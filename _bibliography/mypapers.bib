@ARTICLE{Hosseinzadeh_Kassani2019-vz,
  title    = "{Multimodal Sparse Classifier for Adolescent Brain Age Prediction}",
  author   = "Hosseinzadeh Kassani, Peyman and Gossmann, Alexej and Wang,
              Yu-Ping",
  abstract = "The study of healthy brain development helps to better understand
              both brain transformation and connectivity patterns, which happen
              during childhood to adulthood. This study presents a sparse
              machine learning solution across whole-brain functional
              connectivity (FC) measures of three data sets, derived from
              resting state functional magnetic resonance imaging (rs-fMRI) and
              two task fMRI data including a working memory n-back task
              (nb-fMRI) and an emotion identification task (em-fMRI). The fMRI
              data are collected from the Philadelphia Neurodevelopmental
              Cohort (PNC) for the prediction of brain age in adolescents. Due
              to extremely large variable-to-instance ratio of PNC data, a high
              dimensional matrix with several irrelevant and highly correlated
              features is generated, and hence a sparse learning approach is
              necessary to extract effective features from fMRI data. We
              propose a sparse learner based on the residual errors along the
              estimation of an inverse problem for extreme learning machine
              (ELM). Our proposed method is able to overcome the overlearning
              problem by pruning several redundant features and their
              corresponding output weights. The proposed multimodal sparse ELM
              classifier based on residual errors (RES-ELM) is highly
              competitive in terms of classification accuracy compared to its
              counterparts such as conventional ELM, and sparse Bayesian
              learning ELM.",
  journal  = "IEEE journal of biomedical and health informatics",
  month    =  jun,
  year     =  2019,
  url      = "http://dx.doi.org/10.1109/JBHI.2019.2925710",
  language = "en",
  issn     = "2168-2208, 2168-2194",
  pmid     = "31265424",
  doi      = "10.1109/JBHI.2019.2925710"
}

@ARTICLE{Gossmann2018-yt,
  title    = "{{FDR}-Corrected Sparse Canonical Correlation Analysis with
              Applications to Imaging Genomics}",
  author   = "Gossmann, Alexej and Zille, Pascal and Calhoun, Vince and
              Wang, Yu-Ping",
  journal  = "IEEE Transactions on Medical Imaging",
  volume   =  37,
  number   =  8,
  pages    = "1761--1774",
  month    =  aug,
  year     =  2018,
  url      = "http://dx.doi.org/10.1109/TMI.2018.2815583",
  keywords = "Bioinformatics;Brain;Correlation;Functional magnetic resonance
              imaging;Genomics;Testing;Genome;Machine learning;Probabilistic
              and statistical methods;fMRI analysis",
  language = "en",
  issn     = "0278-0062, 1558-254X",
  pmid     = "29993802",
  doi      = "10.1109/TMI.2018.2815583",
  archivePrefix = "arXiv",
  eprint        = "1705.04312",
  primaryClass  = "stat.ME",
  arxiv         = "1705.04312",
  github        = "agisga/FDRcorrectedSCCA"
}

@ARTICLE{Gossmann2017-yu,
  title    = "{A sparse regression method for group-wise feature selection with
              false discovery rate control}",
  author   = "Gossmann, A and Cao, S and Brzyski, D and Zhao, L J and Deng, H W
              and Wang, Y P",
  journal  = "IEEE/ACM Transactions on Computational Biology and Bioinformatics
              / IEEE, ACM",
  volume   =  15,
  number   =  4,
  pages    = "1066--1078",
  month    =  jul,
  year     =  2018,
  url      = "http://dx.doi.org/10.1109/TCBB.2017.2780106",
  keywords = "Bioinformatics;Correlation;DNA;Estimation;Feature
              extraction;Genomics;Mathematical model;G.3 Probability and
              Statistics;G.3.b Correlation and regression analysis;G.3.n
              Statistical computing;I.2.6.g Machine learning;J.2.h Mathematics
              and statistics;J.3.a Biology and genetics",
  language = "en",
  issn     = "1545-5963, 1557-9964",
  pmid     = "29990279",
  doi      = "10.1109/TCBB.2017.2780106",
  github   = "agisga/grpSLOPEMC"
}

@ARTICLE{Brzyski2018-as,
  title     = "{Group {SLOPE} -- Adaptive Selection of Groups of Predictors}",
  author    = "Brzyski, Damian and Gossmann, Alexej and Su, Weijie and Bogdan,
               Ma{\l}gorzata",
  journal   = "Journal of the American Statistical Association",
  publisher = "Taylor \& Francis",
  pages     = "1--15",
  month     =  jan,
  year      =  2018,
  url       = "https://doi.org/10.1080/01621459.2017.1411269",
  issn      = "0162-1459",
  doi       = "10.1080/01621459.2017.1411269",
  arxiv     = {1610.04960},
  cran      = "https://cran.r-project.org/package=grpSLOPE",
  github    = "agisga/grpSLOPE"
}


@article{sammarco2015,
  title={Hyperbaric Oxygen Promotes Proximal Bone Regeneration and Organized Collagen Composition during Digit Regeneration},
  author={Sammarco, Mimi C and Simkin, Jennifer and Cammack, Alexander J and Fassler, Danielle and Gossmann, Alexej and Marrero, Luis and Lacey, Michelle and Van Meter, Keith and Muneoka, Ken},
  journal={PloS one},
  volume={10},
  number={10},
  year={2015},
  publisher={Public Library of Science},
  doi={10.1371/journal.pone.0140156}
}

@article{cao2015,
  title={Unified tests for fine scale mapping and identifying sparse high-dimensional sequence associations},
  author={Cao, Shaolong and Qin, Huaizhen and Gossmann, Alexej and Deng, Hong-Wen and Wang, Yu-Ping},
  journal={Bioinformatics},
  volume   =  32,
  number   =  3,
  pages    = "330--337",
  year     =  2016,
  month    =  feb,
  language = "en",
  url      = "http://dx.doi.org/10.1093/bioinformatics/btv586",
  issn     = "1367-4803, 1367-4811",
  pmid     = "26458888",
  doi      = "10.1093/bioinformatics/btv586",
  pmc      = "PMC5006306"
}

@inproceedings{gossmann2018,
  title      = "{Test data reuse for evaluation of adaptive machine learning
                algorithms: over-fitting to a fixed 'test' dataset and a
                potential solution}",
  booktitle  = "{Medical Imaging 2018: Image Perception, Observer Performance,
                and Technology Assessment}",
  author     = "Gossmann, Alexej and Pezeshk, Aria and Sahiner, Berkman",
  abstract   = "After the initial release of a machine learning algorithm, the
                subsequently gathered data can be used to augment the training
                dataset in order to modify or fine-tune the algorithm. For
                algorithm performance evaluation that generalizes to a targeted
                population of cases, ideally, test datasets randomly drawn from
                the targeted population are used. To ensure that test results
                generalize to new data, the algorithm needs to be evaluated on
                new and independent test data each time a new performance
                evaluation is required. However, medical test datasets of
                sufficient quality are often hard to acquire, and it is
                tempting to utilize a previously-used test dataset for a new
                performance evaluation. With extensive simulation studies, we
                illustrate how such a ``naive'' approach to test data reuse can
                inadvertently result in overfitting the algorithm to the test
                data, even when only a global performance metric is reported
                back from the test dataset. The overfitting behavior leads to a
                loss in generalization and overly optimistic conclusions about
                the algorithm performance. We investigate the use of the
                Thresholdout method of Dwork et. al. (Ref. 1) to tackle this
                problem. Thresholdout allows repeated reuse of the same test
                dataset. It essentially reports a noisy version of the
                performance metric on the test data, and provides theoretical
                guarantees on how many times the test dataset can be accessed
                to ensure generalization of the reported answers to the
                underlying distribution. With extensive simulation studies, we
                show that Thresholdout indeed substantially reduces the problem
                of overfitting to the test data under the simulation
                conditions, at the cost of a mild additional uncertainty on the
                reported test performance. We also extend some of the
                theoretical guarantees to the area under the ROC curve as the
                reported performance metric.",
  publisher  = "International Society for Optics and Photonics",
  month      =  mar,
  year       =  2018,
  url        = "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10577/105770K/Test-data-reuse-for-evaluation-of-adaptive-machine-learning-algorithms/10.1117/12.2293818.short?SSO=1",
  keywords   = "adaptive data analysis; continuous machine learning; data
                reuse; receiver operating characteristic curve (ROC); area
                under the ROC curve (AUC); classification performance;
                Thresholdout; reusable holdout;",
  conference = "Medical Imaging 2018: Image Perception, Observer Performance,
                and Technology Assessment",
  doi        = "10.1117/12.2293818",
  location   = {Houston, Texas},
}

@inproceedings{cao2015,
 author = {Cao, Shaolong and Qin, Huaizhen and Gossmann, Alexej and Deng, Hong-Wen and Wang, Yu-Ping},
 title = {Unified Tests for Fine Scale Mapping and Identifying Sparse High-dimensional Sequence Associations},
 booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
 series = {BCB '15},
 year = {2015},
 isbn = {978-1-4503-3853-0},
 location = {Atlanta, Georgia},
 pages = {241--249},
 numpages = {9},
 doi = {10.1145/2808719.2808744},
 acmid = {2808744},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Framingham heart study, complex relatedness, scaled Lp sparse regression, uHDSet test}
}

@inproceedings{gossmann2015,
 author = {Gossmann, Alexej and Cao, Shaolong and Wang, Yu-Ping},
 title = {Identification of Significant Genetic Variants via SLOPE, and Its Extension to Group SLOPE},
 booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
 series = {BCB '15},
 year = {2015},
 isbn = {978-1-4503-3853-0},
 location = {Atlanta, Georgia},
 pages = {232--240},
 numpages = {9},
 doi = {10.1145/2808719.2808743},
 acmid = {2808743},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LASSO, SLOPE, false discovery rate, group LASSO, sparse regression}
}

@inproceedings{gossmann2019,
  author = {Gossmann, Alexej and Cha, Kenny H. and Sun, Xudong},
  title = {Variational inference based assessment of mammographic lesion classification algorithms under distribution shift},
  booktitle = {Medical Imaging Meets NeurIPS Workshop (MED-NeurIPS) 2019},
  month =  dec,
  year = {2019},
  location = {Vancouver, Canada},
  url = "https://profs.etsmtl.ca/hlombaert/public/medneurips2019/72_CameraReadySubmission_neurips_2019.pdf"
}

@mastersthesis{gossmann2012,
  document_type     = {Bachelor's Thesis},
  author            = {Alexej Gossmann},
  title             = {On disjunction and numerical existence properties of extensions of Heyting arithmetic},
  school            = {Technische Universit\"{a}t Darmstadt},
  year              = {2012},
  type              = {Bachelor Thesis},
  month             = {October},
}

@INPROCEEDINGS{Sun2019-bd,
  title     = "{Variational Resampling Based Assessment of Deep Neural Networks
               under Distribution Shift}",
  booktitle = "{2019 IEEE Symposium Series on Computational Intelligence (SSCI)}",
  author    = "Sun, X and Gossmann, A and Wang, Y and Bischt, B",
  abstract  = "A novel variational inference based resampling framework is
               proposed to evaluate the robustness and generalization
               capability of deep learning models with respect to distribution
               shift. We use Auto Encoding Variational Bayes to find a latent
               representation of the data, on which a Variational Gaussian
               Mixture Model is applied to deliberately create distribution
               shift by dividing the dataset into different clusters.
               Wasserstein distance is used to characterize the extent of
               distribution shift between the generated data splits. In
               experiments using the Fashion- MNIST data, we assess several
               popular image classification Convolutional Neural Network (CNN)
               architectures and Bayesian CNN models with respect to their
               robustness and generalization behavior under the deliberately
               created distribution shift, which is analyzed in contrast to
               random Cross Validation. Our method of creating artificial
               domain splits of a single dataset may also be used to establish
               novel model selection criteria and assessment tools in machine
               learning, as well as for benchmark methods in the areas of
               domain adaptation and domain generalization.",
  pages     = "1344--1353",
  month     =  dec,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/SSCI44817.2019.9002665",
  keywords  = "Machine learning;Data models;Benchmark testing;Robustness;Bayes
               methods;Training;Neural networks;Bayesian CNN;Variational
               Inference;Resampling;Distribution Shift;Wasserstein
               Distance;Domain Adaptation;Domain Generalization;Transfer
               Learning;Model Selection;Cross
               Validation;Generalization;Robustness",
  doi       = "10.1109/SSCI44817.2019.9002665",
  archivePrefix = "arXiv",
  arxiv         = "1906.02972",
  primaryClass  = "cs.LG",
  arxivid       = "1906.02972",
  github        = "compstat-lmu/paper_2019_variationalResampleDistributionShift"
}

@INPROCEEDINGS{Gossmann2020-zx,
  title      = "{Performance deterioration of deep neural networks for lesion
                classification in mammography due to distribution shift: an
                analysis based on artificially created distribution shift}",
  booktitle  = "{Medical Imaging 2020: Computer-Aided Diagnosis}",
  author     = "Gossmann, Alexej and Cha, Kenny H and Sun, Xudong",
  abstract   = "Despite the prominent success of deep learning (DL) in medical
                imaging for tasks such as computer-aided detection and
                diagnosis, the field faces a number of challenging problems. An
                important issue is that of mismatch of data distributions
                between different data sources, also known as a distribution
                shift. Distribution shifts may also be present between
                different subpopulations or subgroups. Distribution shifts that
                are not easily detectable can prevent the successful deployment
                of DL models in medical imaging. We use variational inference
                to create subsets of a given dataset while enforcing artificial
                distribution shifts between these subsets, thus creating
                subsets with different characteristics that represent different
                pseudo ``data sources''. By training and testing ROI-based
                malignant/benign lesion classification models over these pseudo
                data sources, we evaluate the extent to which distribution
                shift could deteriorate the performance of popular DL models.
                We show that distribution shift indeed poses a serious concern
                for malignant/benign lesion classification in mammography, and
                we show that the algorithmically created pseudo data sources
                may not correspond to any recorded clinical or image
                characteristics. This study shows a potential method for
                evaluating deep learning algorithms for robustness against
                distribution shifts. Furthermore, our technique can serve as a
                benchmark method for development of new models which aim to be
                robust to distribution shift.",
  publisher  = "International Society for Optics and Photonics",
  volume     =  11314,
  pages      = "1131404",
  month      =  mar,
  year       =  2020,
  url        = "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11314/1131404/Performance-deterioration-of-deep-neural-networks-for-lesion-classification-in/10.1117/12.2551346.short",
  keywords   = "Deep Learning; Distribution Shift; Domain Adaptation; Domain
                Generalization; Model Selection; Variational Inference;
                Mammography; Breast Lesion Classification;",
  conference = "Medical Imaging 2020: Computer-Aided Diagnosis",
  doi        = "10.1117/12.2551346"
}

@INPROCEEDINGS{Cha2020-de,
  title      = "{Supplementing training with data from a shifted distribution
                for machine learning classifiers: adding more cases may not
                always help}",
  booktitle  = "{Medical Imaging 2020: Image Perception, Observer Performance,
                and Technology Assessment}",
  author     = "Cha, Kenny H and Gossmann, Alexej and Petrick, Nicholas and
                Sahiner, Berkman",
  abstract   = "In this study, we show that when a training data set is
                supplemented by drawing samples from a distribution that is
                different from that of the target population, the differences
                in the distributions of the original and supplemental training
                populations should be considered to maximize the performance of
                the classifier in the target population. Depending on these
                distributions, drawing a large number of cases from the
                supplemental distribution may result in lower performance
                compared to limiting the number of added cases. This is
                relevant for medical images when synthetic data is used for
                training a machine learning algorithm, which may result in a
                mixed distribution for the training set. We simulated a
                twoclass classification problem and determined the performance
                of a linear classifier and a neural network classifier on test
                cases when trained with cases from only the target
                distribution, and when cases from a shifted, supplemental
                distribution are added to a limited number of cases from the
                target distribution. We show that adding data from a
                supplemental distribution for machine learning classifier
                training may improve the performance on the target test
                distribution. However, given the same number of training cases
                from a mixed distribution, the performance may not reach the
                performance of only training on data from the target
                distribution. In addition, the increase in performance will
                peak or plateau, depending on the shift in the distribution and
                the number of cases from the supplemental distribution.",
  publisher  = "International Society for Optics and Photonics",
  volume     =  11316,
  pages      = "113160S",
  month      =  mar,
  year       =  2020,
  url        = "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11316/113160S/Supplementing-training-with-data-from-a-shifted-distribution-for-machine/10.1117/12.2550538.short",
  keywords   = "Machine Learning; Distribution Shifts; Neural Network;
                Classifier; Simulation Study;",
  conference = "Medical Imaging 2020: Image Perception, Observer Performance,
                and Technology Assessment",
  doi        = "10.1117/12.2550538"
}

@ARTICLE{Pennello2020-uw,
  title    = "{Discussion on "Approval policies for modifications to machine
              learning-based software as a medical device: A study of
              bio-creep" by Jean Feng, Scott Emerson, and Noah Simon}",
  author   = "Pennello, Gene and Sahiner, Berkman and Gossmann, Alexej and
              Petrick, Nicholas",
  journal  = "Biometrics",
  month    =  oct,
  year     =  2020,
  url      = "http://dx.doi.org/10.1111/biom.13381",
  language = "en",
  issn     = "0006-341X, 1541-0420",
  pmid     = "33040332",
  doi      = "10.1111/biom.13381"
}

@ARTICLE{Gossmann2021-bp,
  title     = "{Test Data Reuse for the Evaluation of Continuously Evolving
               Classification Algorithms Using the Area under the Receiver
               Operating Characteristic Curve}",
  author    = "Gossmann, Alexej and Pezeshk, Aria and Wang, Yu-Ping and
               Sahiner, Berkman",
  journal   = "SIAM Journal on Mathematics of Data Science",
  publisher = "Society for Industrial and Applied Mathematics",
  pages     = "692--714",
  month     =  jan,
  year      =  2021,
  url       = "https://doi.org/10.1137/20M1333110",
  doi       = "10.1137/20M1333110",
  github    = "DIDSR/ThresholdoutAUC"
}

@article{fengBayesianLogisticRegression2022,
  title = {Bayesian Logistic Regression for Online Recalibration and Revision of Risk Prediction Models with Performance Guarantees},
  author = {Feng, Jean and Gossmann, Alexej and Sahiner, Berkman and Pirracchio, Romain},
  date = {2022-01-11},
  year = 2022,
  journal = {Journal of the American Medical Informatics Association},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocab280},
  url = {https://doi.org/10.1093/jamia/ocab280},
  arxiv = {2110.06866},
  github = {jjfeng/bayesian_model_revision}
}

@inproceedings{fengSequentialAlgorithmicModification2022b,
  title = {Sequential Algorithmic Modification with Test Data Reuse},
  booktitle = {Proceedings of the {{Thirty-Eighth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Feng, Jean and Pennllo, Gene and Petrick, Nicholas and Sahiner, Berkman and Pirracchio, Romain and Gossmann, Alexej},
  year = {2022},
  month = aug,
  pages = {674--684},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {After initial release of a machine learning algorithm, the model can be fine-tuned by retraining on subsequently gathered data, adding newly discovered features, or more. Each modification introduces a risk of deteriorating performance and must be validated on a test dataset. It may not always be practical to assemble a new dataset for testing each modification, especially when most modifications are minor or are implemented in rapid succession. Recent work has shown how one can repeatedly test modifications on the same dataset and protect against overfitting by (i) discretizing test results along a grid and (ii) applying a Bonferroni correction to adjust for the total number of modifications considered by an adaptive developer. However, the standard Bonferroni correction is overly conservative when most modifications are beneficial and/or highly correlated. This work investigates more powerful approaches using alpha-recycling and sequentially-rejective graphical procedures (SRGPs). We introduce two novel extensions that account for correlation between adaptively chosen algorithmic modifications: the first leverages the correlation between consecutive modifications using flexible fixed sequence tests, and the second leverages the correlation between the proposed modifications and those generated by a hypothetical prespecified model updating procedure. In empirical analyses, both SRGPs control the error rate of approving deleterious modifications and approve significantly more beneficial modifications than previous approaches.},
  langid = {english},
  arxiv = {2203.11377},
  github = {jjfeng/adaptive_SRGP}
}


@misc{fengMonitoringMachineLearning2022,
  title = {Monitoring Machine Learning ({{ML}})-Based Risk Prediction Algorithms in the Presence of Confounding Medical Interventions},
  author = {Feng, Jean and Gossmann, Alexej and Pennello, Gene and Petrick, Nicholas and Sahiner, Berkman and Pirracchio, Romain},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09781},
  eprint = {2211.09781},
  arxiv = {2211.09781},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Monitoring the performance of machine learning (ML)-based risk prediction models in healthcare is complicated by the issue of confounding medical interventions (CMI): when an algorithm predicts a patient to be at high risk for an adverse event, clinicians are more likely to administer prophylactic treatment and alter the very target that the algorithm aims to predict. Ignoring CMI by monitoring only the untreated patients--whose outcomes remain unaltered--can inflate false alarm rates, because the evolution of both the model and clinician-ML interactions can induce complex dependencies in the data that violate standard assumptions. A more sophisticated approach is to explicitly account for CMI by modeling treatment propensities, but its time-varying nature makes accurate estimation difficult. Given the many sources of complexity in the data, it is important to determine situations in which a simple procedure that ignores CMI provides valid inference. Here we describe the special case of monitoring model calibration, under either the assumption of conditional exchangeability or time-constant selection bias. We introduce a new score-based cumulative sum (CUSUM) chart for monitoring in a frequentist framework and review an alternative approach using Bayesian inference. Through simulations, we investigate the benefits of combining model updating with monitoring and study when over-trust in a prediction model does (or does not) delay detection. Finally, we simulate monitoring an ML-based postoperative nausea and vomiting risk calculator during the COVID-19 pandemic.},
  archiveprefix = {arXiv},
}


@misc{corollerMultiomicsInvestigationPrognostic2023,
  title = {Multi-Omics Investigation on the Prognostic and Predictive Factors in Metastatic Breast Cancer Using Data from {{Phase III}} Ribociclib Clinical Trials: {{A}} Statistical and Machine Learning Analysis Plan},
  shorttitle = {Multi-Omics Investigation on the Prognostic and Predictive Factors in Metastatic Breast Cancer Using Data from {{Phase III}} Ribociclib Clinical Trials},
  author = {Coroller, Thibaud and Sahiner, Berkman and Amatya, Anup and Gossmann, Alexej and Karagiannis, Konstantinos and Samala, Ravi K. and {Santana-Quintero}, Luis and Solovieff, Nadia and Wang, Craig and {Amiri-Kordestani}, Laleh and Cao, Qian and Cha, Kenny H. and Orbach, Rosane Charlab and Cross, Frank H. and Hu, Tingting and Huang, Ruihao and Kraft, Jeffrey and Krusche, Peter and Li, Yutong and Li, Zheng and Mazo, Ilya and Moloney, Conor and Paul, Rahul and Plawinski, Jason and Schnakenberg, Susan and Serra, Paolo and Smith, Sean and Song, Chi and Su, Fei and Subramaniam, Sajanth and Tiwari, Mohit and Vechery, Colin and Xiong, Xin and Zarate, Juan Pablo and Ziegler, Jonathan and Zhu, Hao and Chakravartty, Arunava and Liu, Qi and Ohlssen, David and Petrick, Nicholas and Schneider, Julie A. and Walderhaug, Mark and Zuber, Emmanuel},
  year = {2023},
  month = aug,
  pages = {2023.08.30.23294367},
  publisher = {{medRxiv}},
  doi = {10.1101/2023.08.30.23294367},
  urldate = {2023-09-17},
  abstract = {In 2020, Novartis Pharmaceuticals Corporation and the U.S. Food and Drug Administration (FDA) started a 4-year scientific collaboration to find novel radiogenomics-based prognostic and predictive factors for HR+/HER2-metastatic breast cancer under a Research Collaboration Agreement. This manuscript aims to detail the guiding principles and methodology for this study. We include a discussion of internal and external clinical, genomics, imaging datasets, data processing workflows, and machine learning model development strategies. We also prospectively define our success criteria to ensure robust scientific outputs. Disclosure This publication reflects the views of the authors and should not be construed to represent FDA's views or policies.},
  archiveprefix = {medRxiv},
  copyright = {\textcopyright{} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
}

@misc{fengThisModelReliable2023,
  title = {Is This Model Reliable for Everyone? {{Testing}} for Strong Calibration},
  shorttitle = {Is This Model Reliable for Everyone?},
  author = {Feng, Jean and Gossmann, Alexej and Pirracchio, Romain and Petrick, Nicholas and Pennello, Gene and Sahiner, Berkman},
  year = {2023},
  month = jul,
  number = {arXiv:2307.15247},
  eprint = {arXiv:2307.15247},
  arxiv = {arXiv:2307.15247},
  github = {jjfeng/testing_strong_calibration},
  publisher = {{arXiv}},
  urldate = {2023-09-17},
  abstract = {In a well-calibrated risk prediction model, the average predicted probability is close to the true event rate for any given subgroup. Such models are reliable across heterogeneous populations and satisfy strong notions of algorithmic fairness. However, the task of auditing a model for strong calibration is well-known to be difficult -- particularly for machine learning (ML) algorithms -- due to the sheer number of potential subgroups. As such, common practice is to only assess calibration with respect to a few predefined subgroups. Recent developments in goodness-of-fit testing offer potential solutions but are not designed for settings with weak signal or where the poorly calibrated subgroup is small, as they either overly subdivide the data or fail to divide the data at all. We introduce a new testing procedure based on the following insight: if we can reorder observations by their expected residuals, there should be a change in the association between the predicted and observed residuals along this sequence if a poorly calibrated subgroup exists. This lets us reframe the problem of calibration testing into one of changepoint detection, for which powerful methods already exist. We begin with introducing a sample-splitting procedure where a portion of the data is used to train a suite of candidate models for predicting the residual, and the remaining data are used to perform a score-based cumulative sum (CUSUM) test. To further improve power, we then extend this adaptive CUSUM test to incorporate cross-validation, while maintaining Type I error control under minimal assumptions. Compared to existing methods, the proposed procedure consistently achieved higher power in simulation studies and more than doubled the power when auditing a mortality risk prediction model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
}

@article{corollerMethodologyGoodMachine2023,
  title = {Methodology for Good Machine Learning with Multi‐omics Data},
  author = {Coroller, Thibaud and Sahiner, Berkman and Amatya, Anup and Gossmann, Alexej and Karagiannis, Konstantinos and Moloney, Conor and Samala, Ravi K. and Santana‐Quintero, Luis and Solovieff, Nadia and Wang, Craig and Amiri‐Kordestani, Laleh and Cao, Qian and Cha, Kenny H. and Charlab, Rosane and Cross Jr, Frank H. and Hu, Tingting and Huang, Ruihao and Kraft, Jeffrey and Krusche, Peter and Li, Yutong and Li, Zheng and Mazo, Ilya and Paul, Rahul and Schnakenberg, Susan and Serra, Paolo and Smith, Sean and Song, Chi and Su, Fei and Tiwari, Mohit and Vechery, Colin and Xiong, Xin and Zarate, Juan Pablo and Zhu, Hao and Chakravartty, Arunava and Liu, Qi and Ohlssen, David and Petrick, Nicholas and Schneider, Julie A. and Walderhaug, Mark and Zuber, Emmanuel},
  date = {2023-11-15},
  year = {2023},
  month = nov,
  journaltitle = {Clinical Pharmacology \& Therapeutics},
  shortjournal = {Clin Pharma and Therapeutics},
  pages = {cpt.3105},
  issn = {0009-9236, 1532-6535},
  doi = {10.1002/cpt.3105},
}

@inproceedings{feng2023towards,
  title = {Towards a Post-Market Monitoring Framework for Machine Learning-Based Medical Devices: {{A}} Case Study},
  booktitle = {{{NeurIPS}} 2023 Workshop on Regulatable {{ML}}},
  author = {Feng, Jean and Subbaswamy, Adarsh and Gossmann, Alexej and Singh, Harvineet and Sahiner, Berkman and Kim, Mi-Ok and Pennello, Gene and Petrick, Nicholas and Pirracchio, Romain and Xia, Fan},
  date = {2023},
  year = {2023},
  month = dec,
  arxiv = {2311.11463},
  url = {https://openreview.net/forum?id=L97dqPfQdT},
}

@incollection{sidulovaDeepUnsupervisedClustering2023,
  title = {Deep {{Unsupervised Clustering}} for {{Conditional Identification}} of {{Subgroups Within}} a {{Digital Pathology Image Set}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} – {{MICCAI}} 2023},
  author = {Sidulova, Mariia and Sun, Xudong and Gossmann, Alexej},
  editor = {Greenspan, Hayit and Madabhushi, Anant and Mousavi, Parvin and Salcudean, Septimiu and Duncan, James and Syeda-Mahmood, Tanveer and Taylor, Russell},
  date = {2023},
  year = {2023},
  month = nov,
  volume = {14227},
  pages = {666--675},
  publisher = {{Springer Nature Switzerland}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-43993-3_64},
  isbn = {978-3-031-43992-6 978-3-031-43993-3},
}

@incollection{zotero-4436,
  title = {Considerations in the Assessment of Machine Learning Algorithm Performance for Medical Imaging},
  booktitle = {Deep {{Learning}} for {{Medical Image Analysis}}},
  author = {Gossmann, Alexej and Sahiner, Berkman and Samala, Ravi K. and Wen, Si and Cha, Kenny H. and Petrick, Nicholas},
  editor = {Zhou, Kevin S. and Greenspan, Hayit and Shen, Dinggang},
  series = {The {{Elsevier}} and {{MICCAI Society Book Series}}},
  edition = {2},
  pages = {473--507},
  year = {2023},
  month = dec,
  publisher = {{Elsevier Academic Press}},
  isbn = {978-0-323-85124-4},
  doi = {10.1016/B978-0-32-385124-4.00029-5},
}
