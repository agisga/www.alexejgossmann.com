@ARTICLE{Hosseinzadeh_Kassani2019-vz,
  title    = "{Multimodal Sparse Classifier for Adolescent Brain Age Prediction}",
  author   = "Hosseinzadeh Kassani, Peyman and Gossmann, Alexej and Wang,
              Yu-Ping",
  abstract = "The study of healthy brain development helps to better understand
              both brain transformation and connectivity patterns, which happen
              during childhood to adulthood. This study presents a sparse
              machine learning solution across whole-brain functional
              connectivity (FC) measures of three data sets, derived from
              resting state functional magnetic resonance imaging (rs-fMRI) and
              two task fMRI data including a working memory n-back task
              (nb-fMRI) and an emotion identification task (em-fMRI). The fMRI
              data are collected from the Philadelphia Neurodevelopmental
              Cohort (PNC) for the prediction of brain age in adolescents. Due
              to extremely large variable-to-instance ratio of PNC data, a high
              dimensional matrix with several irrelevant and highly correlated
              features is generated, and hence a sparse learning approach is
              necessary to extract effective features from fMRI data. We
              propose a sparse learner based on the residual errors along the
              estimation of an inverse problem for extreme learning machine
              (ELM). Our proposed method is able to overcome the overlearning
              problem by pruning several redundant features and their
              corresponding output weights. The proposed multimodal sparse ELM
              classifier based on residual errors (RES-ELM) is highly
              competitive in terms of classification accuracy compared to its
              counterparts such as conventional ELM, and sparse Bayesian
              learning ELM.",
  journal  = "IEEE journal of biomedical and health informatics",
  month    =  jun,
  year     =  2019,
  url      = "http://dx.doi.org/10.1109/JBHI.2019.2925710",
  language = "en",
  issn     = "2168-2208, 2168-2194",
  pmid     = "31265424",
  doi      = "10.1109/JBHI.2019.2925710"
}

@ARTICLE{Gossmann2018-yt,
  title    = "{{FDR}-Corrected Sparse Canonical Correlation Analysis with
              Applications to Imaging Genomics}",
  author   = "Gossmann, Alexej and Zille, Pascal and Calhoun, Vince and
              Wang, Yu-Ping",
  journal  = "IEEE Transactions on Medical Imaging",
  volume   =  37,
  number   =  8,
  pages    = "1761--1774",
  month    =  aug,
  year     =  2018,
  url      = "http://dx.doi.org/10.1109/TMI.2018.2815583",
  keywords = "Bioinformatics;Brain;Correlation;Functional magnetic resonance
              imaging;Genomics;Testing;Genome;Machine learning;Probabilistic
              and statistical methods;fMRI analysis",
  language = "en",
  issn     = "0278-0062, 1558-254X",
  pmid     = "29993802",
  doi      = "10.1109/TMI.2018.2815583",
  archivePrefix = "arXiv",
  eprint        = "1705.04312",
  primaryClass  = "stat.ME",
  arxiv         = "1705.04312",
  github        = "agisga/FDRcorrectedSCCA"
}

@ARTICLE{Gossmann2017-yu,
  title    = "{A sparse regression method for group-wise feature selection with
              false discovery rate control}",
  author   = "Gossmann, A and Cao, S and Brzyski, D and Zhao, L J and Deng, H W
              and Wang, Y P",
  journal  = "IEEE/ACM Transactions on Computational Biology and Bioinformatics
              / IEEE, ACM",
  volume   =  15,
  number   =  4,
  pages    = "1066--1078",
  month    =  jul,
  year     =  2018,
  url      = "http://dx.doi.org/10.1109/TCBB.2017.2780106",
  keywords = "Bioinformatics;Correlation;DNA;Estimation;Feature
              extraction;Genomics;Mathematical model;G.3 Probability and
              Statistics;G.3.b Correlation and regression analysis;G.3.n
              Statistical computing;I.2.6.g Machine learning;J.2.h Mathematics
              and statistics;J.3.a Biology and genetics",
  language = "en",
  issn     = "1545-5963, 1557-9964",
  pmid     = "29990279",
  doi      = "10.1109/TCBB.2017.2780106",
  github   = "agisga/grpSLOPEMC"
}

@ARTICLE{Brzyski2018-as,
  title     = "{Group {SLOPE} -- Adaptive Selection of Groups of Predictors}",
  author    = "Brzyski, Damian and Gossmann, Alexej and Su, Weijie and Bogdan,
               Ma{\l}gorzata",
  journal   = "Journal of the American Statistical Association",
  publisher = "Taylor \& Francis",
  pages     = "1--15",
  month     =  jan,
  year      =  2018,
  url       = "https://doi.org/10.1080/01621459.2017.1411269",
  issn      = "0162-1459",
  doi       = "10.1080/01621459.2017.1411269",
  arxiv     = {1610.04960},
  cran      = "https://cran.r-project.org/package=grpSLOPE",
  github    = "agisga/grpSLOPE"
}


@article{sammarco2015,
  title={Hyperbaric Oxygen Promotes Proximal Bone Regeneration and Organized Collagen Composition during Digit Regeneration},
  author={Sammarco, Mimi C and Simkin, Jennifer and Cammack, Alexander J and Fassler, Danielle and Gossmann, Alexej and Marrero, Luis and Lacey, Michelle and Van Meter, Keith and Muneoka, Ken},
  journal={PloS one},
  volume={10},
  number={10},
  year={2015},
  publisher={Public Library of Science},
  doi={10.1371/journal.pone.0140156}
}

@article{cao2015,
  title={Unified tests for fine scale mapping and identifying sparse high-dimensional sequence associations},
  author={Cao, Shaolong and Qin, Huaizhen and Gossmann, Alexej and Deng, Hong-Wen and Wang, Yu-Ping},
  journal={Bioinformatics},
  volume   =  32,
  number   =  3,
  pages    = "330--337",
  year     =  2016,
  month    =  feb,
  language = "en",
  url      = "http://dx.doi.org/10.1093/bioinformatics/btv586",
  issn     = "1367-4803, 1367-4811",
  pmid     = "26458888",
  doi      = "10.1093/bioinformatics/btv586",
  pmc      = "PMC5006306"
}

@inproceedings{gossmann2018,
  title      = "{Test data reuse for evaluation of adaptive machine learning
                algorithms: over-fitting to a fixed 'test' dataset and a
                potential solution}",
  booktitle  = "{Medical Imaging 2018: Image Perception, Observer Performance,
                and Technology Assessment}",
  author     = "Gossmann, Alexej and Pezeshk, Aria and Sahiner, Berkman",
  abstract   = "After the initial release of a machine learning algorithm, the
                subsequently gathered data can be used to augment the training
                dataset in order to modify or fine-tune the algorithm. For
                algorithm performance evaluation that generalizes to a targeted
                population of cases, ideally, test datasets randomly drawn from
                the targeted population are used. To ensure that test results
                generalize to new data, the algorithm needs to be evaluated on
                new and independent test data each time a new performance
                evaluation is required. However, medical test datasets of
                sufficient quality are often hard to acquire, and it is
                tempting to utilize a previously-used test dataset for a new
                performance evaluation. With extensive simulation studies, we
                illustrate how such a ``naive'' approach to test data reuse can
                inadvertently result in overfitting the algorithm to the test
                data, even when only a global performance metric is reported
                back from the test dataset. The overfitting behavior leads to a
                loss in generalization and overly optimistic conclusions about
                the algorithm performance. We investigate the use of the
                Thresholdout method of Dwork et. al. (Ref. 1) to tackle this
                problem. Thresholdout allows repeated reuse of the same test
                dataset. It essentially reports a noisy version of the
                performance metric on the test data, and provides theoretical
                guarantees on how many times the test dataset can be accessed
                to ensure generalization of the reported answers to the
                underlying distribution. With extensive simulation studies, we
                show that Thresholdout indeed substantially reduces the problem
                of overfitting to the test data under the simulation
                conditions, at the cost of a mild additional uncertainty on the
                reported test performance. We also extend some of the
                theoretical guarantees to the area under the ROC curve as the
                reported performance metric.",
  publisher  = "International Society for Optics and Photonics",
  month      =  mar,
  year       =  2018,
  url        = "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10577/105770K/Test-data-reuse-for-evaluation-of-adaptive-machine-learning-algorithms/10.1117/12.2293818.short?SSO=1",
  keywords   = "adaptive data analysis; continuous machine learning; data
                reuse; receiver operating characteristic curve (ROC); area
                under the ROC curve (AUC); classification performance;
                Thresholdout; reusable holdout;",
  conference = "Medical Imaging 2018: Image Perception, Observer Performance,
                and Technology Assessment",
  doi        = "10.1117/12.2293818",
  location   = {Houston, Texas},
}
% TODO: reenable/move to new entry when journal paper is accepted
%github     = "DIDSR/ThresholdoutAUC"

@inproceedings{cao2015,
 author = {Cao, Shaolong and Qin, Huaizhen and Gossmann, Alexej and Deng, Hong-Wen and Wang, Yu-Ping},
 title = {Unified Tests for Fine Scale Mapping and Identifying Sparse High-dimensional Sequence Associations},
 booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
 series = {BCB '15},
 year = {2015},
 isbn = {978-1-4503-3853-0},
 location = {Atlanta, Georgia},
 pages = {241--249},
 numpages = {9},
 doi = {10.1145/2808719.2808744},
 acmid = {2808744},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Framingham heart study, complex relatedness, scaled Lp sparse regression, uHDSet test}
}

@inproceedings{gossmann2015,
 author = {Gossmann, Alexej and Cao, Shaolong and Wang, Yu-Ping},
 title = {Identification of Significant Genetic Variants via SLOPE, and Its Extension to Group SLOPE},
 booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
 series = {BCB '15},
 year = {2015},
 isbn = {978-1-4503-3853-0},
 location = {Atlanta, Georgia},
 pages = {232--240},
 numpages = {9},
 doi = {10.1145/2808719.2808743},
 acmid = {2808743},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LASSO, SLOPE, false discovery rate, group LASSO, sparse regression}
}

@inproceedings{gossmann2019,
  author = {Gossmann, Alexej and Cha, Kenny H. and Sun, Xudong},
  title = {Variational inference based assessment of mammographic lesion classification algorithms under distribution shift},
  booktitle = {Medical Imaging Meets NeurIPS Workshop (MED-NeurIPS) 2019},
  month =  dec,
  year = {2019},
  location = {Vancouver, Canada},
  url = "https://profs.etsmtl.ca/hlombaert/public/medneurips2019/72_CameraReadySubmission_neurips_2019.pdf"
}

@mastersthesis{gossmann2012,
  document_type     = {Bachelor's Thesis},
  author            = {Alexej Gossmann},
  title             = {On disjunction and numerical existence properties of extensions of Heyting arithmetic},
  school            = {Technische Universit\"{a}t Darmstadt},
  year              = {2012},
  type              = {Bachelor Thesis},
  month             = {October},
}

@INPROCEEDINGS{Sun2019-bd,
  title     = "{Variational Resampling Based Assessment of Deep Neural Networks
               under Distribution Shift}",
  booktitle = "{2019 IEEE Symposium Series on Computational Intelligence (SSCI)}",
  author    = "Sun, X and Gossmann, A and Wang, Y and Bischt, B",
  abstract  = "A novel variational inference based resampling framework is
               proposed to evaluate the robustness and generalization
               capability of deep learning models with respect to distribution
               shift. We use Auto Encoding Variational Bayes to find a latent
               representation of the data, on which a Variational Gaussian
               Mixture Model is applied to deliberately create distribution
               shift by dividing the dataset into different clusters.
               Wasserstein distance is used to characterize the extent of
               distribution shift between the generated data splits. In
               experiments using the Fashion- MNIST data, we assess several
               popular image classification Convolutional Neural Network (CNN)
               architectures and Bayesian CNN models with respect to their
               robustness and generalization behavior under the deliberately
               created distribution shift, which is analyzed in contrast to
               random Cross Validation. Our method of creating artificial
               domain splits of a single dataset may also be used to establish
               novel model selection criteria and assessment tools in machine
               learning, as well as for benchmark methods in the areas of
               domain adaptation and domain generalization.",
  pages     = "1344--1353",
  month     =  dec,
  year      =  2019,
  url       = "http://dx.doi.org/10.1109/SSCI44817.2019.9002665",
  keywords  = "Machine learning;Data models;Benchmark testing;Robustness;Bayes
               methods;Training;Neural networks;Bayesian CNN;Variational
               Inference;Resampling;Distribution Shift;Wasserstein
               Distance;Domain Adaptation;Domain Generalization;Transfer
               Learning;Model Selection;Cross
               Validation;Generalization;Robustness",
  doi       = "10.1109/SSCI44817.2019.9002665",
  archivePrefix = "arXiv",
  arxiv         = "1906.02972",
  primaryClass  = "cs.LG",
  arxivid       = "1906.02972",
  github        = "compstat-lmu/paper_2019_variationalResampleDistributionShift"
}
